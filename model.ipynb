{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# source code is from https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py\n",
    "\n",
    "# coding: utf-8\n",
    "# pylint: disable=too-many-arguments, too-many-locals, invalid-name, fixme, E0012, R0912, C0302\n",
    "\"\"\"Scikit-Learn Wrapper interface for XGBoost.\"\"\"\n",
    "import copy\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "from .core import Booster, DMatrix, XGBoostError\n",
    "from .training import train\n",
    "\n",
    "# Do not use class names on scikit-learn directly.  Re-define the classes on\n",
    "# .compat to guarantee the behavior without scikit-learn\n",
    "from .compat import (SKLEARN_INSTALLED, XGBModelBase,\n",
    "                     XGBClassifierBase, XGBRegressorBase, XGBoostLabelEncoder)\n",
    "\n",
    "\n",
    "def _objective_decorator(func):\n",
    "    \"\"\"Decorate an objective function\n",
    "    Converts an objective function using the typical sklearn metrics\n",
    "    signature so that it is usable with ``xgboost.training.train``\n",
    "    Parameters\n",
    "    ----------\n",
    "    func: callable\n",
    "        Expects a callable with signature ``func(y_true, y_pred)``:\n",
    "        y_true: array_like of shape [n_samples]\n",
    "            The target values\n",
    "        y_pred: array_like of shape [n_samples]\n",
    "            The predicted values\n",
    "    Returns\n",
    "    -------\n",
    "    new_func: callable\n",
    "        The new objective function as expected by ``xgboost.training.train``.\n",
    "        The signature is ``new_func(preds, dmatrix)``:\n",
    "        preds: array_like, shape [n_samples]\n",
    "            The predicted values\n",
    "        dmatrix: ``DMatrix``\n",
    "            The training set from which the labels will be extracted using\n",
    "            ``dmatrix.get_label()``\n",
    "    \"\"\"\n",
    "    def inner(preds, dmatrix):\n",
    "        \"\"\"internal function\"\"\"\n",
    "        labels = dmatrix.get_label()\n",
    "        return func(labels, preds)\n",
    "    return inner\n",
    "\n",
    "\n",
    "__estimator_doc = '''\n",
    "    n_estimators : int\n",
    "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
    "        rounds.\n",
    "'''\n",
    "\n",
    "__model_doc = '''\n",
    "    max_depth : int\n",
    "        Maximum tree depth for base learners.\n",
    "    learning_rate : float\n",
    "        Boosting learning rate (xgb's \"eta\")\n",
    "    verbosity : int\n",
    "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "    objective : string or callable\n",
    "        Specify the learning task and the corresponding learning objective or\n",
    "        a custom objective function to be used (see note below).\n",
    "    booster: string\n",
    "        Specify which booster to use: gbtree, gblinear or dart.\n",
    "    tree_method: string\n",
    "        Specify which tree method to use.  Default to auto.  If this parameter\n",
    "        is set to default, XGBoost will choose the most conservative option\n",
    "        available.  It's recommended to study this option from parameters\n",
    "        document.\n",
    "    n_jobs : int\n",
    "        Number of parallel threads used to run xgboost.\n",
    "    gamma : float\n",
    "        Minimum loss reduction required to make a further partition on a leaf\n",
    "        node of the tree.\n",
    "    min_child_weight : int\n",
    "        Minimum sum of instance weight(hessian) needed in a child.\n",
    "    max_delta_step : int\n",
    "        Maximum delta step we allow each tree's weight estimation to be.\n",
    "    subsample : float\n",
    "        Subsample ratio of the training instance.\n",
    "    colsample_bytree : float\n",
    "        Subsample ratio of columns when constructing each tree.\n",
    "    colsample_bylevel : float\n",
    "        Subsample ratio of columns for each level.\n",
    "    colsample_bynode : float\n",
    "        Subsample ratio of columns for each split.\n",
    "    reg_alpha : float (xgb's alpha)\n",
    "        L1 regularization term on weights\n",
    "    reg_lambda : float (xgb's lambda)\n",
    "        L2 regularization term on weights\n",
    "    scale_pos_weight : float\n",
    "        Balancing of positive and negative weights.\n",
    "    base_score:\n",
    "        The initial prediction score of all instances, global bias.\n",
    "    random_state : int\n",
    "        Random number seed.\n",
    "        .. note::\n",
    "           Using gblinear booster with shotgun updater is nondeterministic as\n",
    "           it uses Hogwild algorithm.\n",
    "    missing : float, default np.nan\n",
    "        Value in the data which needs to be present as a missing value.\n",
    "    num_parallel_tree: int\n",
    "        Used for boosting random forest.\n",
    "    monotone_constraints : str\n",
    "        Constraint of variable monotonicity.  See tutorial for more\n",
    "        information.\n",
    "    interaction_constraints : str\n",
    "        Constraints for interaction representing permitted interactions.  The\n",
    "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
    "        [2, 3, 4]], where each inner list is a group of indices of features\n",
    "        that are allowed to interact with each other.  See tutorial for more\n",
    "        information\n",
    "    importance_type: string, default \"gain\"\n",
    "        The feature importance type for the feature_importances\\\\_ property:\n",
    "        either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
    "    \\\\*\\\\*kwargs : dict, optional\n",
    "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
    "        parameters can be found here:\n",
    "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
    "        Attempting to set a parameter via the constructor args and \\\\*\\\\*kwargs\n",
    "        dict simultaneously will result in a TypeError.\n",
    "        .. note:: \\\\*\\\\*kwargs unsupported by scikit-learn\n",
    "            \\\\*\\\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
    "            that parameters passed via this argument will interact properly\n",
    "            with scikit-learn.\n",
    "'''\n",
    "\n",
    "__custom_obj_note = '''\n",
    "        .. note::  Custom objective function\n",
    "            A custom objective function can be provided for the ``objective``\n",
    "            parameter. In this case, it should have the signature\n",
    "            ``objective(y_true, y_pred) -> grad, hess``:\n",
    "            y_true: array_like of shape [n_samples]\n",
    "                The target values\n",
    "            y_pred: array_like of shape [n_samples]\n",
    "                The predicted values\n",
    "            grad: array_like of shape [n_samples]\n",
    "                The value of the gradient for each sample point.\n",
    "            hess: array_like of shape [n_samples]\n",
    "                The value of the second derivative for each sample point\n",
    "'''\n",
    "\n",
    "\n",
    "def xgboost_model_doc(header, items, extra_parameters=None, end_note=None):\n",
    "    '''Obtain documentation for Scikit-Learn wrappers\n",
    "    Parameters\n",
    "    ----------\n",
    "    header: str\n",
    "       An introducion to the class.\n",
    "    items : list\n",
    "       A list of commom doc items.  Available items are:\n",
    "         - estimators: the meaning of n_estimators\n",
    "         - model: All the other parameters\n",
    "         - objective: note for customized objective\n",
    "    extra_parameters: str\n",
    "       Document for class specific parameters, placed at the head.\n",
    "    end_note: str\n",
    "       Extra notes put to the end.\n",
    "'''\n",
    "    def get_doc(item):\n",
    "        '''Return selected item'''\n",
    "        __doc = {'estimators': __estimator_doc,\n",
    "                 'model': __model_doc,\n",
    "                 'objective': __custom_obj_note}\n",
    "        return __doc[item]\n",
    "\n",
    "    def adddoc(cls):\n",
    "        doc = ['''\n",
    "Parameters\n",
    "----------\n",
    "''']\n",
    "        if extra_parameters:\n",
    "            doc.append(extra_parameters)\n",
    "        doc.extend([get_doc(i) for i in items])\n",
    "        if end_note:\n",
    "            doc.append(end_note)\n",
    "        full_doc = [header + '\\n\\n']\n",
    "        full_doc.extend(doc)\n",
    "        cls.__doc__ = ''.join(full_doc)\n",
    "        return cls\n",
    "    return adddoc\n",
    "\n",
    "\n",
    "@xgboost_model_doc(\"\"\"Implementation of the Scikit-Learn API for XGBoost.\"\"\",\n",
    "                   ['estimators', 'model', 'objective'])\n",
    "class XGBModel(XGBModelBase):\n",
    "    # pylint: disable=too-many-arguments, too-many-instance-attributes, missing-docstring\n",
    "    def __init__(self, max_depth=None, learning_rate=None, n_estimators=100,\n",
    "                 verbosity=None, objective=None, booster=None,\n",
    "                 tree_method=None, n_jobs=None, gamma=None,\n",
    "                 min_child_weight=None, max_delta_step=None, subsample=None,\n",
    "                 colsample_bytree=None, colsample_bylevel=None,\n",
    "                 colsample_bynode=None, reg_alpha=None, reg_lambda=None,\n",
    "                 scale_pos_weight=None, base_score=None, random_state=None,\n",
    "                 missing=np.nan, num_parallel_tree=None,\n",
    "                 monotone_constraints=None, interaction_constraints=None,\n",
    "                 importance_type=\"gain\", gpu_id=None,\n",
    "                 validate_parameters=None, **kwargs):\n",
    "        if not SKLEARN_INSTALLED:\n",
    "            raise XGBoostError(\n",
    "                'sklearn needs to be installed in order to use this module')\n",
    "        self.n_estimators = n_estimators\n",
    "        self.objective = objective\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbosity = verbosity\n",
    "        self.booster = booster\n",
    "        self.tree_method = tree_method\n",
    "        self.gamma = gamma\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.max_delta_step = max_delta_step\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.colsample_bylevel = colsample_bylevel\n",
    "        self.colsample_bynode = colsample_bynode\n",
    "        self.reg_alpha = reg_alpha\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.scale_pos_weight = scale_pos_weight\n",
    "        self.base_score = base_score\n",
    "        self.missing = missing\n",
    "        self.num_parallel_tree = num_parallel_tree\n",
    "        self.kwargs = kwargs\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.monotone_constraints = monotone_constraints\n",
    "        self.interaction_constraints = interaction_constraints\n",
    "        self.importance_type = importance_type\n",
    "        self.gpu_id = gpu_id\n",
    "        self.validate_parameters = validate_parameters\n",
    "\n",
    "    def _more_tags(self):\n",
    "        '''Tags used for scikit-learn data validation.'''\n",
    "        return {'allow_nan': True}\n",
    "\n",
    "    def get_booster(self):\n",
    "        \"\"\"Get the underlying xgboost Booster of this model.\n",
    "        This will raise an exception when fit was not called\n",
    "        Returns\n",
    "        -------\n",
    "        booster : a xgboost booster of underlying model\n",
    "        \"\"\"\n",
    "        if not hasattr(self, '_Booster'):\n",
    "            raise XGBoostError('need to call fit or load_model beforehand')\n",
    "        return self._Booster\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.  Modification of the sklearn method to\n",
    "        allow unknown kwargs. This allows using the full range of xgboost\n",
    "        parameters that are not defined as member variables in sklearn grid\n",
    "        search.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "\n",
    "        # this concatenates kwargs into paraemters, enabling `get_params` for\n",
    "        # obtaining parameters from keyword paraemters.\n",
    "        for key, value in params.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                self.kwargs[key] = value\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # pylint: disable=attribute-defined-outside-init\n",
    "        \"\"\"Get parameters.\"\"\"\n",
    "        # Based on: https://stackoverflow.com/questions/59248211\n",
    "        # The basic flow in `get_params` is:\n",
    "        # 0. Return parameters in subclass first, by using inspect.\n",
    "        # 1. Return parameters in `XGBModel` (the base class).\n",
    "        # 2. Return whatever in `**kwargs`.\n",
    "        # 3. Merge them.\n",
    "        params = super().get_params(deep)\n",
    "        cp = copy.copy(self)\n",
    "        cp.__class__ = cp.__class__.__bases__[0]\n",
    "        params.update(cp.__class__.get_params(cp, deep))\n",
    "        # if kwargs is a dict, update params accordingly\n",
    "        if isinstance(self.kwargs, dict):\n",
    "            params.update(self.kwargs)\n",
    "        if isinstance(params['random_state'], np.random.RandomState):\n",
    "            params['random_state'] = params['random_state'].randint(\n",
    "                np.iinfo(np.int32).max)\n",
    "\n",
    "        def parse_parameter(value):\n",
    "            for t in (int, float, str):\n",
    "                try:\n",
    "                    ret = t(value)\n",
    "                    return ret\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            return None\n",
    "\n",
    "        # Get internal parameter values\n",
    "        try:\n",
    "            config = json.loads(self.get_booster().save_config())\n",
    "            stack = [config]\n",
    "            internal = {}\n",
    "            while stack:\n",
    "                obj = stack.pop()\n",
    "                for k, v in obj.items():\n",
    "                    if k.endswith('_param'):\n",
    "                        for p_k, p_v in v.items():\n",
    "                            internal[p_k] = p_v\n",
    "                    elif isinstance(v, dict):\n",
    "                        stack.append(v)\n",
    "\n",
    "            for k, v in internal.items():\n",
    "                if k in params.keys() and params[k] is None:\n",
    "                    params[k] = parse_parameter(v)\n",
    "        except XGBoostError:\n",
    "            pass\n",
    "        return params\n",
    "\n",
    "    def get_xgb_params(self):\n",
    "        \"\"\"Get xgboost specific parameters.\"\"\"\n",
    "        params = self.get_params()\n",
    "        # Parameters that should not go into native learner.\n",
    "        wrapper_specific = {\n",
    "            'importance_type', 'kwargs', 'missing', 'n_estimators'}\n",
    "        filtered = dict()\n",
    "        for k, v in params.items():\n",
    "            if k not in wrapper_specific:\n",
    "                filtered[k] = v\n",
    "        return filtered\n",
    "\n",
    "    def get_num_boosting_rounds(self):\n",
    "        \"\"\"Gets the number of xgboost boosting rounds.\"\"\"\n",
    "        return self.n_estimators\n",
    "\n",
    "    def save_model(self, fname: str):\n",
    "        \"\"\"Save the model to a file.\n",
    "        The model is saved in an XGBoost internal format which is universal\n",
    "        among the various XGBoost interfaces. Auxiliary attributes of the\n",
    "        Python Booster object (such as feature names) will not be saved.\n",
    "          .. note::\n",
    "            See:\n",
    "            https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : string\n",
    "            Output file name\n",
    "        \"\"\"\n",
    "        meta = dict()\n",
    "        for k, v in self.__dict__.items():\n",
    "            if k == '_le':\n",
    "                meta['_le'] = self._le.to_json()\n",
    "                continue\n",
    "            if k == '_Booster':\n",
    "                continue\n",
    "            if k == 'classes_':\n",
    "                # numpy array is not JSON serializable\n",
    "                meta['classes_'] = self.classes_.tolist()\n",
    "                continue\n",
    "            try:\n",
    "                json.dumps({k: v})\n",
    "                meta[k] = v\n",
    "            except TypeError:\n",
    "                warnings.warn(str(k) + ' is not saved in Scikit-Learn meta.')\n",
    "        meta['type'] = type(self).__name__\n",
    "        meta = json.dumps(meta)\n",
    "        self.get_booster().set_attr(scikit_learn=meta)\n",
    "        self.get_booster().save_model(fname)\n",
    "        # Delete the attribute after save\n",
    "        self.get_booster().set_attr(scikit_learn=None)\n",
    "\n",
    "    def load_model(self, fname):\n",
    "        # pylint: disable=attribute-defined-outside-init\n",
    "        \"\"\"Load the model from a file.\n",
    "        The model is loaded from an XGBoost internal format which is universal\n",
    "        among the various XGBoost interfaces. Auxiliary attributes of the\n",
    "        Python Booster object (such as feature names) will not be loaded.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : string\n",
    "            Input file name.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, '_Booster'):\n",
    "            self._Booster = Booster({'n_jobs': self.n_jobs})\n",
    "        self._Booster.load_model(fname)\n",
    "        meta = self._Booster.attr('scikit_learn')\n",
    "        if meta is None:\n",
    "            warnings.warn(\n",
    "                'Loading a native XGBoost model with Scikit-Learn interface.')\n",
    "            return\n",
    "        meta = json.loads(meta)\n",
    "        states = dict()\n",
    "        for k, v in meta.items():\n",
    "            if k == '_le':\n",
    "                self._le = XGBoostLabelEncoder()\n",
    "                self._le.from_json(v)\n",
    "                continue\n",
    "            if k == 'classes_':\n",
    "                self.classes_ = np.array(v)\n",
    "                continue\n",
    "            if k == 'type' and type(self).__name__ != v:\n",
    "                msg = 'Current model type: {}, '.format(type(self).__name__) + \\\n",
    "                      'type of model in file: {}'.format(v)\n",
    "                raise TypeError(msg)\n",
    "            if k == 'type':\n",
    "                continue\n",
    "            states[k] = v\n",
    "        self.__dict__.update(states)\n",
    "        # Delete the attribute after load\n",
    "        self.get_booster().set_attr(scikit_learn=None)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, base_margin=None,\n",
    "            eval_set=None, eval_metric=None, early_stopping_rounds=None,\n",
    "            verbose=True, xgb_model=None, sample_weight_eval_set=None,\n",
    "            callbacks=None):\n",
    "        # pylint: disable=invalid-name,attribute-defined-outside-init\n",
    "        \"\"\"Fit gradient boosting model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like\n",
    "            Feature matrix\n",
    "        y : array_like\n",
    "            Labels\n",
    "        sample_weight : array_like\n",
    "            instance weights\n",
    "        base_margin : array_like\n",
    "            global bias for each instance.\n",
    "        eval_set : list, optional\n",
    "            A list of (X, y) tuple pairs to use as validation sets, for which\n",
    "            metrics will be computed.\n",
    "            Validation metrics will help us track the performance of the model.\n",
    "        sample_weight_eval_set : list, optional\n",
    "            A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
    "            instance weights on the i-th validation set.\n",
    "        eval_metric : str, list of str, or callable, optional\n",
    "            If a str, should be a built-in evaluation metric to use. See\n",
    "            doc/parameter.rst.\n",
    "            If a list of str, should be the list of multiple built-in evaluation metrics\n",
    "            to use.\n",
    "            If callable, a custom evaluation metric. The call\n",
    "            signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a\n",
    "            DMatrix object such that you may need to call the ``get_label``\n",
    "            method. It must return a str, value pair where the str is a name\n",
    "            for the evaluation and value is the value of the evaluation\n",
    "            function. The callable custom objective is always minimized.\n",
    "        early_stopping_rounds : int\n",
    "            Activates early stopping. Validation metric needs to improve at least once in\n",
    "            every **early_stopping_rounds** round(s) to continue training.\n",
    "            Requires at least one item in **eval_set**.\n",
    "            The method returns the model from the last iteration (not the best one).\n",
    "            If there's more than one item in **eval_set**, the last entry will be used\n",
    "            for early stopping.\n",
    "            If there's more than one metric in **eval_metric**, the last metric will be\n",
    "            used for early stopping.\n",
    "            If early stopping occurs, the model will have three additional fields:\n",
    "            ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
    "        verbose : bool\n",
    "            If `verbose` and an evaluation set is used, writes the evaluation\n",
    "            metric measured on the validation set to stderr.\n",
    "        xgb_model : str\n",
    "            file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
    "            loaded before training (allows training continuation).\n",
    "        callbacks : list of callback functions\n",
    "            List of callback functions that are applied at end of each iteration.\n",
    "            It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
    "            Example:\n",
    "            .. code-block:: python\n",
    "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
    "        \"\"\"\n",
    "        train_dmatrix = DMatrix(data=X, label=y, weight=sample_weight,\n",
    "                                base_margin=base_margin,\n",
    "                                missing=self.missing,\n",
    "                                nthread=self.n_jobs)\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        if eval_set is not None:\n",
    "            if not isinstance(eval_set[0], (list, tuple)):\n",
    "                raise TypeError('Unexpected input type for `eval_set`')\n",
    "            if sample_weight_eval_set is None:\n",
    "                sample_weight_eval_set = [None] * len(eval_set)\n",
    "            else:\n",
    "                assert len(eval_set) == len(sample_weight_eval_set)\n",
    "            evals = list(\n",
    "                DMatrix(eval_set[i][0], label=eval_set[i][1], missing=self.missing,\n",
    "                        weight=sample_weight_eval_set[i], nthread=self.n_jobs)\n",
    "                for i in range(len(eval_set)))\n",
    "            evals = list(zip(evals, [\"validation_{}\".format(i) for i in\n",
    "                                     range(len(evals))]))\n",
    "        else:\n",
    "            evals = ()\n",
    "\n",
    "        params = self.get_xgb_params()\n",
    "\n",
    "        if callable(self.objective):\n",
    "            obj = _objective_decorator(self.objective)\n",
    "            params[\"objective\"] = \"reg:squarederror\"\n",
    "        else:\n",
    "            obj = None\n",
    "\n",
    "        feval = eval_metric if callable(eval_metric) else None\n",
    "        if eval_metric is not None:\n",
    "            if callable(eval_metric):\n",
    "                eval_metric = None\n",
    "            else:\n",
    "                params.update({'eval_metric': eval_metric})\n",
    "\n",
    "        self._Booster = train(params, train_dmatrix,\n",
    "                              self.get_num_boosting_rounds(), evals=evals,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals_result=evals_result, obj=obj, feval=feval,\n",
    "                              verbose_eval=verbose, xgb_model=xgb_model,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "        if evals_result:\n",
    "            for val in evals_result.items():\n",
    "                evals_result_key = list(val[1].keys())[0]\n",
    "                evals_result[val[0]][evals_result_key] = val[1][\n",
    "                    evals_result_key]\n",
    "            self.evals_result_ = evals_result\n",
    "\n",
    "        if early_stopping_rounds is not None:\n",
    "            self.best_score = self._Booster.best_score\n",
    "            self.best_iteration = self._Booster.best_iteration\n",
    "            self.best_ntree_limit = self._Booster.best_ntree_limit\n",
    "        return self\n",
    "\n",
    "    def predict(self, data, output_margin=False, ntree_limit=None,\n",
    "                validate_features=True, base_margin=None):\n",
    "        \"\"\"\n",
    "        Predict with `data`.\n",
    "        .. note:: This function is not thread safe.\n",
    "          For each booster object, predict can only be called from one thread.\n",
    "          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
    "          of model object and then call ``predict()``.\n",
    "          .. code-block:: python\n",
    "            preds = bst.predict(dtest, ntree_limit=num_round)\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : numpy.array/scipy.sparse\n",
    "            Data to predict with\n",
    "        output_margin : bool\n",
    "            Whether to output the raw untransformed margin value.\n",
    "        ntree_limit : int\n",
    "            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
    "            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
    "        validate_features : bool\n",
    "            When this is True, validate that the Booster's and data's feature_names are identical.\n",
    "            Otherwise, it is assumed that the feature_names are the same.\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : numpy array\n",
    "        \"\"\"\n",
    "        # pylint: disable=missing-docstring,invalid-name\n",
    "        test_dmatrix = DMatrix(data, base_margin=base_margin,\n",
    "                               missing=self.missing, nthread=self.n_jobs)\n",
    "        # get ntree_limit to use - if none specified, default to\n",
    "        # best_ntree_limit if defined, otherwise 0.\n",
    "        if ntree_limit is None:\n",
    "            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\n",
    "        return self.get_booster().predict(test_dmatrix,\n",
    "                                          output_margin=output_margin,\n",
    "                                          ntree_limit=ntree_limit,\n",
    "                                          validate_features=validate_features)\n",
    "\n",
    "    def apply(self, X, ntree_limit=0):\n",
    "        \"\"\"Return the predicted leaf every tree for each sample.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like, shape=[n_samples, n_features]\n",
    "            Input features matrix.\n",
    "        ntree_limit : int\n",
    "            Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
    "        Returns\n",
    "        -------\n",
    "        X_leaves : array_like, shape=[n_samples, n_trees]\n",
    "            For each datapoint x in X and for each tree, return the index of the\n",
    "            leaf x ends up in. Leaves are numbered within\n",
    "            ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
    "        \"\"\"\n",
    "        test_dmatrix = DMatrix(X, missing=self.missing, nthread=self.n_jobs)\n",
    "        return self.get_booster().predict(test_dmatrix,\n",
    "                                          pred_leaf=True,\n",
    "                                          ntree_limit=ntree_limit)\n",
    "\n",
    "    def evals_result(self):\n",
    "        \"\"\"Return the evaluation results.\n",
    "        If **eval_set** is passed to the `fit` function, you can call\n",
    "        ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
    "        When **eval_metric** is also passed to the `fit` function, the\n",
    "        **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
    "        Returns\n",
    "        -------\n",
    "        evals_result : dictionary\n",
    "        Example\n",
    "        -------\n",
    "        .. code-block:: python\n",
    "            param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
    "            clf = xgb.XGBModel(**param_dist)\n",
    "            clf.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                    eval_metric='logloss',\n",
    "                    verbose=True)\n",
    "            evals_result = clf.evals_result()\n",
    "        The variable **evals_result** will contain:\n",
    "        .. code-block:: python\n",
    "            {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
    "            'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
    "        \"\"\"\n",
    "        if self.evals_result_:\n",
    "            evals_result = self.evals_result_\n",
    "        else:\n",
    "            raise XGBoostError('No results.')\n",
    "\n",
    "        return evals_result\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"\n",
    "        Feature importances property\n",
    "        .. note:: Feature importance is defined only for tree boosters\n",
    "            Feature importance is only defined when the decision tree model is chosen as base\n",
    "            learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
    "            as linear learners (`booster=gblinear`).\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances_ : array of shape ``[n_features]``\n",
    "        \"\"\"\n",
    "        if self.get_params()['booster'] not in {'gbtree', 'dart'}:\n",
    "            raise AttributeError(\n",
    "                'Feature importance is not defined for Booster type {}'\n",
    "                .format(self.booster))\n",
    "        b = self.get_booster()\n",
    "        score = b.get_score(importance_type=self.importance_type)\n",
    "        all_features = [score.get(f, 0.) for f in b.feature_names]\n",
    "        all_features = np.array(all_features, dtype=np.float32)\n",
    "        return all_features / all_features.sum()\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        \"\"\"\n",
    "        Coefficients property\n",
    "        .. note:: Coefficients are defined only for linear learners\n",
    "            Coefficients are only defined when the linear model is chosen as\n",
    "            base learner (`booster=gblinear`). It is not defined for other base\n",
    "            learner types, such as tree learners (`booster=gbtree`).\n",
    "        Returns\n",
    "        -------\n",
    "        coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
    "        \"\"\"\n",
    "        if self.get_params()['booster'] != 'gblinear':\n",
    "            raise AttributeError(\n",
    "                'Coefficients are not defined for Booster type {}'\n",
    "                .format(self.booster))\n",
    "        b = self.get_booster()\n",
    "        coef = np.array(json.loads(\n",
    "            b.get_dump(dump_format='json')[0])['weight'])\n",
    "        # Logic for multiclass classification\n",
    "        n_classes = getattr(self, 'n_classes_', None)\n",
    "        if n_classes is not None:\n",
    "            if n_classes > 2:\n",
    "                assert len(coef.shape) == 1\n",
    "                assert coef.shape[0] % n_classes == 0\n",
    "                coef = coef.reshape((n_classes, -1))\n",
    "        return coef\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        \"\"\"\n",
    "        Intercept (bias) property\n",
    "        .. note:: Intercept is defined only for linear learners\n",
    "            Intercept (bias) is only defined when the linear model is chosen as base\n",
    "            learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
    "            as tree learners (`booster=gbtree`).\n",
    "        Returns\n",
    "        -------\n",
    "        intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
    "        \"\"\"\n",
    "        if self.get_params()['booster'] != 'gblinear':\n",
    "            raise AttributeError(\n",
    "                'Intercept (bias) is not defined for Booster type {}'\n",
    "                .format(self.booster))\n",
    "        b = self.get_booster()\n",
    "        return np.array(json.loads(b.get_dump(dump_format='json')[0])['bias'])\n",
    "\n",
    "\n",
    "@xgboost_model_doc(\n",
    "    \"Implementation of the scikit-learn API for XGBoost classification.\",\n",
    "    ['model', 'objective'])\n",
    "class XGBClassifier(XGBModel, XGBClassifierBase):\n",
    "    # pylint: disable=missing-docstring,invalid-name,too-many-instance-attributes\n",
    "    def __init__(self, objective=\"binary:logistic\", **kwargs):\n",
    "        super().__init__(objective=objective, **kwargs)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, base_margin=None,\n",
    "            eval_set=None, eval_metric=None,\n",
    "            early_stopping_rounds=None, verbose=True, xgb_model=None,\n",
    "            sample_weight_eval_set=None, callbacks=None):\n",
    "        # pylint: disable = attribute-defined-outside-init,arguments-differ\n",
    "\n",
    "        evals_result = {}\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        xgb_options = self.get_xgb_params()\n",
    "\n",
    "        if callable(self.objective):\n",
    "            obj = _objective_decorator(self.objective)\n",
    "            # Use default value. Is it really not used ?\n",
    "            xgb_options[\"objective\"] = \"binary:logistic\"\n",
    "        else:\n",
    "            obj = None\n",
    "\n",
    "        if self.n_classes_ > 2:\n",
    "            # Switch to using a multiclass objective in the underlying\n",
    "            # XGB instance\n",
    "            xgb_options['objective'] = 'multi:softprob'\n",
    "            xgb_options['num_class'] = self.n_classes_\n",
    "\n",
    "        feval = eval_metric if callable(eval_metric) else None\n",
    "        if eval_metric is not None:\n",
    "            if callable(eval_metric):\n",
    "                eval_metric = None\n",
    "            else:\n",
    "                xgb_options.update({\"eval_metric\": eval_metric})\n",
    "\n",
    "        self._le = XGBoostLabelEncoder().fit(y)\n",
    "        training_labels = self._le.transform(y)\n",
    "\n",
    "        if eval_set is not None:\n",
    "            if sample_weight_eval_set is None:\n",
    "                sample_weight_eval_set = [None] * len(eval_set)\n",
    "            else:\n",
    "                assert len(sample_weight_eval_set) == len(eval_set)\n",
    "            evals = list(\n",
    "                DMatrix(eval_set[i][0],\n",
    "                        label=self._le.transform(eval_set[i][1]),\n",
    "                        missing=self.missing, weight=sample_weight_eval_set[i],\n",
    "                        nthread=self.n_jobs)\n",
    "                for i in range(len(eval_set))\n",
    "            )\n",
    "            nevals = len(evals)\n",
    "            eval_names = [\"validation_{}\".format(i) for i in range(nevals)]\n",
    "            evals = list(zip(evals, eval_names))\n",
    "        else:\n",
    "            evals = ()\n",
    "\n",
    "        if len(X.shape) != 2:\n",
    "            # Simply raise an error here since there might be many\n",
    "            # different ways of reshaping\n",
    "            raise ValueError(\n",
    "                'Please reshape the input data X into 2-dimensional matrix.')\n",
    "        self._features_count = X.shape[1]\n",
    "        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,\n",
    "                                base_margin=base_margin,\n",
    "                                missing=self.missing, nthread=self.n_jobs)\n",
    "\n",
    "        self._Booster = train(xgb_options, train_dmatrix,\n",
    "                              self.get_num_boosting_rounds(),\n",
    "                              evals=evals,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals_result=evals_result, obj=obj, feval=feval,\n",
    "                              verbose_eval=verbose, xgb_model=xgb_model,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "        self.objective = xgb_options[\"objective\"]\n",
    "        if evals_result:\n",
    "            for val in evals_result.items():\n",
    "                evals_result_key = list(val[1].keys())[0]\n",
    "                evals_result[val[0]][\n",
    "                    evals_result_key] = val[1][evals_result_key]\n",
    "            self.evals_result_ = evals_result\n",
    "\n",
    "        if early_stopping_rounds is not None:\n",
    "            self.best_score = self._Booster.best_score\n",
    "            self.best_iteration = self._Booster.best_iteration\n",
    "            self.best_ntree_limit = self._Booster.best_ntree_limit\n",
    "\n",
    "        return self\n",
    "\n",
    "    fit.__doc__ = XGBModel.fit.__doc__.replace(\n",
    "        'Fit gradient boosting model',\n",
    "        'Fit gradient boosting classifier', 1)\n",
    "\n",
    "    def predict(self, data, output_margin=False, ntree_limit=None,\n",
    "                validate_features=True, base_margin=None):\n",
    "        \"\"\"\n",
    "        Predict with `data`.\n",
    "        .. note:: This function is not thread safe.\n",
    "          For each booster object, predict can only be called from one thread.\n",
    "          If you want to run prediction using multiple thread, call\n",
    "          ``xgb.copy()`` to make copies of model object and then call\n",
    "          ``predict()``.\n",
    "          .. code-block:: python\n",
    "            preds = bst.predict(dtest, ntree_limit=num_round)\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array_like\n",
    "            The dmatrix storing the input.\n",
    "        output_margin : bool\n",
    "            Whether to output the raw untransformed margin value.\n",
    "        ntree_limit : int\n",
    "            Limit number of trees in the prediction; defaults to\n",
    "            best_ntree_limit if defined (i.e. it has been trained with early\n",
    "            stopping), otherwise 0 (use all trees).\n",
    "        validate_features : bool\n",
    "            When this is True, validate that the Booster's and data's\n",
    "            feature_names are identical.  Otherwise, it is assumed that the\n",
    "            feature_names are the same.\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : numpy array\n",
    "        \"\"\"\n",
    "        test_dmatrix = DMatrix(data, base_margin=base_margin,\n",
    "                               missing=self.missing, nthread=self.n_jobs)\n",
    "        if ntree_limit is None:\n",
    "            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\n",
    "        class_probs = self.get_booster().predict(\n",
    "            test_dmatrix,\n",
    "            output_margin=output_margin,\n",
    "            ntree_limit=ntree_limit,\n",
    "            validate_features=validate_features)\n",
    "        if output_margin:\n",
    "            # If output_margin is active, simply return the scores\n",
    "            return class_probs\n",
    "\n",
    "        if len(class_probs.shape) > 1:\n",
    "            column_indexes = np.argmax(class_probs, axis=1)\n",
    "        else:\n",
    "            column_indexes = np.repeat(0, class_probs.shape[0])\n",
    "            column_indexes[class_probs > 0.5] = 1\n",
    "\n",
    "        if hasattr(self, '_le'):\n",
    "            return self._le.inverse_transform(column_indexes)\n",
    "        warnings.warn(\n",
    "            'Label encoder is not defined.  Returning class probability.')\n",
    "        return class_probs\n",
    "\n",
    "    def predict_proba(self, data, ntree_limit=None, validate_features=True,\n",
    "                      base_margin=None):\n",
    "        \"\"\"\n",
    "        Predict the probability of each `data` example being of a given class.\n",
    "        .. note:: This function is not thread safe\n",
    "            For each booster object, predict can only be called from one\n",
    "            thread.  If you want to run prediction using multiple thread, call\n",
    "            ``xgb.copy()`` to make copies of model object and then call predict\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DMatrix\n",
    "            The dmatrix storing the input.\n",
    "        ntree_limit : int\n",
    "            Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
    "            (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
    "        validate_features : bool\n",
    "            When this is True, validate that the Booster's and data's feature_names are identical.\n",
    "            Otherwise, it is assumed that the feature_names are the same.\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : numpy array\n",
    "            a numpy array with the probability of each data example being of a given class.\n",
    "        \"\"\"\n",
    "        test_dmatrix = DMatrix(data, base_margin=base_margin,\n",
    "                               missing=self.missing, nthread=self.n_jobs)\n",
    "        if ntree_limit is None:\n",
    "            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\n",
    "        class_probs = self.get_booster().predict(test_dmatrix,\n",
    "                                                 ntree_limit=ntree_limit,\n",
    "                                                 validate_features=validate_features)\n",
    "        if self.objective == \"multi:softprob\":\n",
    "            return class_probs\n",
    "        classone_probs = class_probs\n",
    "        classzero_probs = 1.0 - classone_probs\n",
    "        return np.vstack((classzero_probs, classone_probs)).transpose()\n",
    "\n",
    "    def evals_result(self):\n",
    "        \"\"\"Return the evaluation results.\n",
    "        If **eval_set** is passed to the `fit` function, you can call\n",
    "        ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
    "        When **eval_metric** is also passed to the `fit` function, the\n",
    "        **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
    "        Returns\n",
    "        -------\n",
    "        evals_result : dictionary\n",
    "        Example\n",
    "        -------\n",
    "        .. code-block:: python\n",
    "            param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
    "            clf = xgb.XGBClassifier(**param_dist)\n",
    "            clf.fit(X_train, y_train,\n",
    "                    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                    eval_metric='logloss',\n",
    "                    verbose=True)\n",
    "            evals_result = clf.evals_result()\n",
    "        The variable **evals_result** will contain\n",
    "        .. code-block:: python\n",
    "            {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
    "            'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
    "        \"\"\"\n",
    "        if self.evals_result_:\n",
    "            evals_result = self.evals_result_\n",
    "        else:\n",
    "            raise XGBoostError('No results.')\n",
    "\n",
    "        return evals_result\n",
    "\n",
    "\n",
    "@xgboost_model_doc(\n",
    "    \"scikit-learn API for XGBoost random forest classification.\",\n",
    "    ['model', 'objective'],\n",
    "    extra_parameters='''\n",
    "    n_estimators : int\n",
    "        Number of trees in random forest to fit.\n",
    "''')\n",
    "class XGBRFClassifier(XGBClassifier):\n",
    "    # pylint: disable=missing-docstring\n",
    "    def __init__(self,\n",
    "                 learning_rate=1,\n",
    "                 subsample=0.8,\n",
    "                 colsample_bynode=0.8,\n",
    "                 reg_lambda=1e-5,\n",
    "                 **kwargs):\n",
    "        super().__init__(learning_rate=learning_rate,\n",
    "                         subsample=subsample,\n",
    "                         colsample_bynode=colsample_bynode,\n",
    "                         reg_lambda=reg_lambda,\n",
    "                         **kwargs)\n",
    "\n",
    "    def get_xgb_params(self):\n",
    "        params = super(XGBRFClassifier, self).get_xgb_params()\n",
    "        params['num_parallel_tree'] = self.n_estimators\n",
    "        return params\n",
    "\n",
    "    def get_num_boosting_rounds(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "@xgboost_model_doc(\n",
    "    \"Implementation of the scikit-learn API for XGBoost regression.\",\n",
    "    ['estimators', 'model', 'objective'])\n",
    "class XGBRegressor(XGBModel, XGBRegressorBase):\n",
    "    # pylint: disable=missing-docstring\n",
    "    def __init__(self, objective=\"reg:squarederror\", **kwargs):\n",
    "        super().__init__(objective=objective, **kwargs)\n",
    "\n",
    "\n",
    "@xgboost_model_doc(\n",
    "    \"scikit-learn API for XGBoost random forest regression.\",\n",
    "    ['model', 'objective'])\n",
    "class XGBRFRegressor(XGBRegressor):\n",
    "    # pylint: disable=missing-docstring\n",
    "    def __init__(self, learning_rate=1, subsample=0.8, colsample_bynode=0.8,\n",
    "                 reg_lambda=1e-5, **kwargs):\n",
    "        super().__init__(learning_rate=learning_rate, subsample=subsample,\n",
    "                         colsample_bynode=colsample_bynode,\n",
    "                         reg_lambda=reg_lambda, **kwargs)\n",
    "\n",
    "    def get_xgb_params(self):\n",
    "        params = super(XGBRFRegressor, self).get_xgb_params()\n",
    "        params['num_parallel_tree'] = self.n_estimators\n",
    "        return params\n",
    "\n",
    "    def get_num_boosting_rounds(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "@xgboost_model_doc(\n",
    "    'Implementation of the Scikit-Learn API for XGBoost Ranking.',\n",
    "    ['estimators', 'model'],\n",
    "    end_note='''\n",
    "        Note\n",
    "        ----\n",
    "        A custom objective function is currently not supported by XGBRanker.\n",
    "        Likewise, a custom metric function is not supported either.\n",
    "        Note\n",
    "        ----\n",
    "        Query group information is required for ranking tasks.\n",
    "        Before fitting the model, your data need to be sorted by query\n",
    "        group. When fitting the model, you need to provide an additional array\n",
    "        that contains the size of each query group.\n",
    "        For example, if your original data look like:\n",
    "        +-------+-----------+---------------+\n",
    "        |   qid |   label   |   features    |\n",
    "        +-------+-----------+---------------+\n",
    "        |   1   |   0       |   x_1         |\n",
    "        +-------+-----------+---------------+\n",
    "        |   1   |   1       |   x_2         |\n",
    "        +-------+-----------+---------------+\n",
    "        |   1   |   0       |   x_3         |\n",
    "        +-------+-----------+---------------+\n",
    "        |   2   |   0       |   x_4         |\n",
    "        +-------+-----------+---------------+\n",
    "        |   2   |   1       |   x_5         |\n",
    "        +-------+-----------+---------------+\n",
    "        |   2   |   1       |   x_6         |\n",
    "        +-------+-----------+---------------+\n",
    "        |   2   |   1       |   x_7         |\n",
    "        +-------+-----------+---------------+\n",
    "        then your group array should be ``[3, 4]``.\n",
    "''')\n",
    "class XGBRanker(XGBModel):\n",
    "    # pylint: disable=missing-docstring,too-many-arguments,invalid-name\n",
    "    def __init__(self, objective='rank:pairwise', **kwargs):\n",
    "        super().__init__(objective=objective, **kwargs)\n",
    "        if callable(self.objective):\n",
    "            raise ValueError(\n",
    "                \"custom objective function not supported by XGBRanker\")\n",
    "        if \"rank:\" not in self.objective:\n",
    "            raise ValueError(\"please use XGBRanker for ranking task\")\n",
    "\n",
    "    def fit(self, X, y, group, sample_weight=None, base_margin=None,\n",
    "            eval_set=None,\n",
    "            sample_weight_eval_set=None, eval_group=None, eval_metric=None,\n",
    "            early_stopping_rounds=None, verbose=False, xgb_model=None,\n",
    "            callbacks=None):\n",
    "        # pylint: disable = attribute-defined-outside-init,arguments-differ\n",
    "        \"\"\"Fit gradient boosting ranker\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like\n",
    "            Feature matrix\n",
    "        y : array_like\n",
    "            Labels\n",
    "        group : array_like\n",
    "            Size of each query group of training data. Should have as many\n",
    "            elements as the query groups in the training data\n",
    "        sample_weight : array_like\n",
    "            Query group weights\n",
    "            .. note:: Weights are per-group for ranking tasks\n",
    "                In ranking task, one weight is assigned to each query group\n",
    "                (not each data point). This is because we only care about the\n",
    "                relative ordering of data points within each group, so it\n",
    "                doesn't make sense to assign weights to individual data points.\n",
    "        base_margin : array_like\n",
    "            Global bias for each instance.\n",
    "        eval_set : list, optional\n",
    "            A list of (X, y) tuple pairs to use as validation sets, for which\n",
    "            metrics will be computed.\n",
    "            Validation metrics will help us track the performance of the model.\n",
    "        sample_weight_eval_set : list, optional\n",
    "            A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
    "            group weights on the i-th validation set.\n",
    "            .. note:: Weights are per-group for ranking tasks\n",
    "                In ranking task, one weight is assigned to each query group (not each\n",
    "                data point). This is because we only care about the relative ordering of\n",
    "                data points within each group, so it doesn't make sense to assign\n",
    "                weights to individual data points.\n",
    "        eval_group : list of arrays, optional\n",
    "            A list in which ``eval_group[i]`` is the list containing the sizes of all\n",
    "            query groups in the ``i``-th pair in **eval_set**.\n",
    "        eval_metric : str, list of str, optional\n",
    "            If a str, should be a built-in evaluation metric to use. See\n",
    "            doc/parameter.rst.\n",
    "            If a list of str, should be the list of multiple built-in evaluation metrics\n",
    "            to use. The custom evaluation metric is not yet supported for the ranker.\n",
    "        early_stopping_rounds : int\n",
    "            Activates early stopping. Validation metric needs to improve at least once in\n",
    "            every **early_stopping_rounds** round(s) to continue training.\n",
    "            Requires at least one item in **eval_set**.\n",
    "            The method returns the model from the last iteration (not the best one).\n",
    "            If there's more than one item in **eval_set**, the last entry will be used\n",
    "            for early stopping.\n",
    "            If there's more than one metric in **eval_metric**, the last metric\n",
    "            will be used for early stopping.\n",
    "            If early stopping occurs, the model will have three additional\n",
    "            fields: ``clf.best_score``, ``clf.best_iteration`` and\n",
    "            ``clf.best_ntree_limit``.\n",
    "        verbose : bool\n",
    "            If `verbose` and an evaluation set is used, writes the evaluation\n",
    "            metric measured on the validation set to stderr.\n",
    "        xgb_model : str\n",
    "            file name of stored XGBoost model or 'Booster' instance XGBoost\n",
    "            model to be loaded before training (allows training continuation).\n",
    "        callbacks : list of callback functions\n",
    "            List of callback functions that are applied at end of each\n",
    "            iteration.  It is possible to use predefined callbacks by using\n",
    "            :ref:`callback_api`.  Example:\n",
    "            .. code-block:: python\n",
    "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
    "        \"\"\"\n",
    "        # check if group information is provided\n",
    "        if group is None:\n",
    "            raise ValueError(\"group is required for ranking task\")\n",
    "\n",
    "        if eval_set is not None:\n",
    "            if eval_group is None:\n",
    "                raise ValueError(\n",
    "                    \"eval_group is required if eval_set is not None\")\n",
    "            if len(eval_group) != len(eval_set):\n",
    "                raise ValueError(\n",
    "                    \"length of eval_group should match that of eval_set\")\n",
    "            if any(group is None for group in eval_group):\n",
    "                raise ValueError(\n",
    "                    \"group is required for all eval datasets for ranking task\")\n",
    "\n",
    "        def _dmat_init(group, **params):\n",
    "            ret = DMatrix(**params)\n",
    "            ret.set_group(group)\n",
    "            return ret\n",
    "\n",
    "        train_dmatrix = DMatrix(data=X, label=y, weight=sample_weight,\n",
    "                                base_margin=base_margin,\n",
    "                                missing=self.missing, nthread=self.n_jobs)\n",
    "        train_dmatrix.set_group(group)\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        if eval_set is not None:\n",
    "            if sample_weight_eval_set is None:\n",
    "                sample_weight_eval_set = [None] * len(eval_set)\n",
    "            evals = [_dmat_init(eval_group[i],\n",
    "                                data=eval_set[i][0],\n",
    "                                label=eval_set[i][1],\n",
    "                                missing=self.missing,\n",
    "                                weight=sample_weight_eval_set[i],\n",
    "                                nthread=self.n_jobs)\n",
    "                     for i in range(len(eval_set))]\n",
    "            nevals = len(evals)\n",
    "            eval_names = [\"eval_{}\".format(i) for i in range(nevals)]\n",
    "            evals = list(zip(evals, eval_names))\n",
    "        else:\n",
    "            evals = ()\n",
    "\n",
    "        params = self.get_xgb_params()\n",
    "\n",
    "        feval = eval_metric if callable(eval_metric) else None\n",
    "        if eval_metric is not None:\n",
    "            if callable(eval_metric):\n",
    "                raise ValueError(\n",
    "                    'Custom evaluation metric is not yet supported for XGBRanker.')\n",
    "            params.update({'eval_metric': eval_metric})\n",
    "\n",
    "        self._Booster = train(params, train_dmatrix,\n",
    "                              self.n_estimators,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals=evals,\n",
    "                              evals_result=evals_result, feval=feval,\n",
    "                              verbose_eval=verbose, xgb_model=xgb_model,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "        self.objective = params[\"objective\"]\n",
    "\n",
    "        if evals_result:\n",
    "            for val in evals_result.items():\n",
    "                evals_result_key = list(val[1].keys())[0]\n",
    "                evals_result[val[0]][evals_result_key] = val[1][evals_result_key]\n",
    "            self.evals_result = evals_result\n",
    "\n",
    "        if early_stopping_rounds is not None:\n",
    "            self.best_score = self._Booster.best_score\n",
    "            self.best_iteration = self._Booster.best_iteration\n",
    "            self.best_ntree_limit = self._Booster.best_ntree_limit\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, data, output_margin=False,\n",
    "                ntree_limit=0, validate_features=True, base_margin=None):\n",
    "\n",
    "        test_dmatrix = DMatrix(data, base_margin=base_margin,\n",
    "                               missing=self.missing)\n",
    "        if ntree_limit is None:\n",
    "            ntree_limit = getattr(self, \"best_ntree_limit\", 0)\n",
    "\n",
    "        return self.get_booster().predict(test_dmatrix,\n",
    "                                          output_margin=output_margin,\n",
    "                                          ntree_limit=ntree_limit,\n",
    "                                          validate_features=validate_features)\n",
    "\n",
    "    predict.__doc__ = XGBModel.predict.__doc__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class rtModel(self, max_depth, eta, gamm, min_child_weight, subsample, silent, objective,\n",
    "                        early_stopping_rounds,\n",
    "                        num_round)\n",
    "\n",
    "    def __init__(self, max_depth, eta, gamma, min_child_weight, subsample, silent, objective,\n",
    "                        early_stopping_rounds,\n",
    "                        num_round):\n",
    "        \n",
    "        super(rtModel, self).__init__()\n",
    "\n",
    "        self.max_depth=max_depth \n",
    "        self.lstm = eta\n",
    "        self.dense = gamma\n",
    "        self.sig = min_child_weight\n",
    "        selsubsample=subsample\n",
    "        self.silent=silent\n",
    "        self.objective=objective\n",
    "        self.early_stopping_rounds=early_stopping_rounds\n",
    "        self.num_rounds=num_rounds\n",
    "    \n",
    "\n",
    "def set_hyperparameters(self, max_depth,\n",
    "                        eta,\n",
    "                        gamma,\n",
    "                        min_child_weight,\n",
    "                        subsample,\n",
    "                        silent,\n",
    "                        objective,\n",
    "                        early_stopping_rounds,\n",
    "                        num_round):\n",
    "    rtm=rtModel.set_hyperparameters(5,0.2,4,6,0.8,0,'binary_logistic', 10,500)\n",
    "    \n",
    "    return rtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}