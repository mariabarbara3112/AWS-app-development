{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Introduction:\n",
    "    \n",
    "The goal is to predict a sentiment from film reviews, to construct and deploy an inference rule(endpoint) and create an App to interact with the models endpoint.\n",
    "\n",
    "After some preprocessing and splitting into training,  validation and test set the data is uploaded to Amazons S3 storage structure. Then the training data is used to train a random tree model (xgboost).\n",
    "\n",
    "The trained model might suffer from under-or overfitting (also called bias-variance-tradeoff), though. To determine the accuracy of the model's pred we have to predict new data; this new data is the part of the dataset that was packed into the test set file. In between the model has been optimized with some hyperparameter tuning, for which another dataset is used which has been put aside during preprocessing.\n",
    "\n",
    "Definitions:\n",
    "0:=negative review\n",
    "1:=positive review\n",
    "    \n",
    "The dataset that is used has been provided by:\n",
    "    \n",
    "Maas, Andrew L., et al. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "QUESTIONS\n",
    "What does review_to_words do?\n",
    "it removes html tags and tokenizes words.\n",
    "\n",
    "What are the five most frequently appearing words?\n",
    " ['movi', 'film', 'one', 'like', 'time']\n",
    "\n",
    "Understanding preprocess_data and convert_and_pad_data:\n",
    "- preprocess_data in this case is called \"pickleData\"; it converts the reviews to tokenized words and removes stopwords, punctuations, html tags and other formatting features and packs the data into a compact format for upload to S3.\n",
    "- at the official github repository from udacity the definition of convert_and_pad data is not given; this seems to be part of another programm.\n",
    "\n",
    "How does this model compare to the XGBoost model?\n",
    "(Answer describes the differences between the RNN model and the XGBoost model and how they perform on the IMDB data.)\n",
    "RNN is an artificial neural network where a cost function is set up and optimized as in the random tree model xgb-boost. The difference is that in the RNN model the parameters are called weights and biases for which error gradients are calculated (how much a change in each weight changes the total error; the adaptations that are made are proportional to the size of their influence on the total error) while the xgb boost model creates different random tree structures to minimize the total error of the predictions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#download and extract the dataset:\n",
    "\n",
    "%mkdir ../data\n",
    "!wget -0../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#then some preprocessing is done:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def loadData(dir='../data/aclImdb'):\n",
    "    reviewsDict = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        reviewsDict[data_type] = {}\n",
    "        ratingsDict[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            reviewsDict[data_type][sentiment] = []\n",
    "            ratingsDict[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for file in files:\n",
    "                with open(file) as review:\n",
    "                    reviewsDict[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    ratingsDict[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(reviewsDict[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} some reviews have no rating\".format(data_type, sentiment)\n",
    "                \n",
    "    return reviewsDict, ratingsDict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviews, ratings = loadData()\n",
    "print(\"Film reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(reviews['train']['pos']), len(reviews['train']['neg']),\n",
    "            len(reviews['test']['pos']), len(reviews['test']['neg'])))\n",
    "\n",
    "['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def randomize(reviews, ratings):\n",
    "    \n",
    "    trainReviews = reviews['train']['pos'] + reviews['train']['neg']\n",
    "    testReviews = reviews['test']['pos'] + reviews['test']['neg']\n",
    "    trainRatings = ratings['train']['pos'] + ratings['train']['neg']\n",
    "    testRatings =ratings['test']['pos'] + ratings['test']['neg']\n",
    "    \n",
    "    #Shuffle data within training and test sets\n",
    "    trainReviews, trainRatings = shuffle(trainReviews, trainRatings)\n",
    "    trainReviews, trainRatings = shuffle(trainReviews, trainRatings)\n",
    "    \n",
    "    return trainReviews,  trainRatings, testReviews, testLabels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviewTraining, reviewTest, reviewTest, ratingTest = randomize(reviews, ratings)\n",
    "print(' Number of reviews in training and test set, respectively': train = {}, test = {}\".format(len(reviewTraining), len(reviewTest)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviewTraining[100]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import re\n",
    "\n",
    "replace = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "replaceWithSpacing = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def convert2words(review):\n",
    "    words = replace.sub(\"\", review.lower())\n",
    "    words = replaceWithSpacing.sub(\" \", words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "convert2words(reviewTraining[100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pickle\n",
    "\n",
    "dirC = os.path.join(\"../cache\", \"sentiment_web_app\")  # storage location\n",
    "os.makedirs(dirC, exist_ok=True) \n",
    "\n",
    "def pickleData(reviewTraining, ratingTraining, reviewTest, ratingTest, dirC=dirC, fileC=\"preprocessed_data.pkl\"):\n",
    "\n",
    "    # If fileC is not None, try to read from it first\n",
    "    fileP = None\n",
    "    if fileC is not None:\n",
    "        try:\n",
    "            with open(os.path.join(dirC, fileC), \"rb\") as file:\n",
    "                fileP = pickle.load(file)\n",
    "            print(\"Read preprocessed data from cache file:\", fileC)\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    # If cache is missing:\n",
    "    if fileP is None:\n",
    "\n",
    "        reviewTrainingW = [convert2words(rev) for rev in reviewTraining]\n",
    "        reviewTestW = [convert2words(rev) for rev in reviewTest]\n",
    "        \n",
    "        # Pickle data for repeated usage:\n",
    "        if fileC is not None:\n",
    "            fileP = dict(reviewTrainingW=reviewTrainingW, reviewTestW=reviewTestW, ratingTraining=ratingTraining, ratingTest=ratingTest)\n",
    "            with open(os.path.join(dirC, fileC), \"wb\") as file:\n",
    "                pickle.dump(fileP, file)\n",
    "            print(\"Wrote preprocessed data to cache file:\", fileC)\n",
    "    else:\n",
    "        # Unpack data \n",
    "        reviewTrainingW, reviewTestW, ratingTraining, ratingTest = (fileP['reviewTrainingW'],\n",
    "                fileP['reviewTestW'], fileP['ratingTraining'], fileP['ratingTest'])\n",
    "    \n",
    "    return reviewTrainingW, reviewTestW, ratingTraining, ratingTest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviewTraining, reviewTest, ratingTest, ratingTest = pickleData(reviewTraining, ratingTraining, reviewTest, ratingTest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def build_dict(rev, nrofwords = 5000):\n",
    "\n",
    "    wrds = [word for r in rev for word in r]\n",
    "    wordCount = dict(Counter(wrds))\n",
    "    swords = sorted(Counter(wrds), key=Counter(wrds).get, reverse=True)\n",
    "    wordDict = {} \n",
    "    for i, w in enumerate(swords[:nrofwords]): \n",
    "        wordDict[w] = i\n",
    "\n",
    "    return wordDict\n",
    "\n",
    "def dict2array(wordDict, sentence, size=500):\n",
    "    words0 = 0 # 'no word' \n",
    "    words1 = 1 # words not appearing in word_dict\n",
    "    revSentence = [words0] * size\n",
    "    for widx, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            revSentence[widx] = wordDict[word]\n",
    "        else:\n",
    "            revSentence[widx] = words1\n",
    "            \n",
    "    return revSentence, min(len(sentence), size)\n",
    "\n",
    "def dict2array_data(wordDict, data, size=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    for sentence in data:\n",
    "        converted, leng = dict2array(wordDict, sentence, size)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)\n",
    "\n",
    "reviewTraining, reviewTrainingLen = dict2array_data(wordDict, train_X)\n",
    "reviewTest, reviewTrainingLen = dict2array_data(wordDict, test_X)\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Alternative implementation( (more concise/shorter)):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def build_dict(reviewTraining, reviewTest, vocabSize=5000, dirC, fileC=\"bow_features.pkl\"):\n",
    "    \n",
    "    # If cfl is not None, try to read from it first\n",
    "    cd = None\n",
    "    if cfl is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cdr, cfl), \"rb\") as file:\n",
    "                cd = joblib.load(file)\n",
    "        except:\n",
    "            pass  # if there are no files in the cache directory further processing has to be done:\n",
    "    \n",
    "    if cd is None:\n",
    "\n",
    "        ctvectorizer = CountVectorizer(max_features=vocabSize)\n",
    "        trainVariables = ctvectorizer.fit_transform(reviewTraining).toarray()\n",
    "        testVariables = ctvectorizer.transform(reviewTest).toarray()\n",
    "        \n",
    "        # store in cache file for future runs \n",
    "        if cfl is not None:\n",
    "            vocab = vectorizer.vocabulary_\n",
    "            cd = dict(trainVariables, testVariables, vocab)\n",
    "            with open(os.path.join(cdr, cfl), \"wb\") as file:\n",
    "                joblib.dump(cd, file)\n",
    "    else:\n",
    "        # data from cache file has to be unpacked:\n",
    "        trainVariables, testVariables, vocab = (cd['trainVariables'],\n",
    "                cd['testVariables'], cd['vocab'])\n",
    "\n",
    "    return trainVariables, testVariables, vocab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviewTraining, reviewTest, vocab = build_words(reviewTraining, reviewTest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(reviewTraining[100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# As the data is in random order it is statistically valid to just take the first thousand samples for validation and the rest # for training\n",
    "\n",
    "xV = pd.DataFrame(reviewTraining[:10000])\n",
    "xTrain = pd.DataFrame(reviewTraining[10000:])\n",
    "\n",
    "yV = pd.DataFrame(ratingTraining[:10000])\n",
    "yTrain = pd.DataFrame(ratingTraining[10000:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dir = '../data/sentiment_web_app'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "pd.DataFrame(reviewTest).to_csv(os.path.join(dir, 'test.csv'), header=False, index=False)\n",
    "pd.concat([yV, xV], axis=1).to_csv(os.path.join(dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([yTrain, xTrain], axis=1).to_csv(os.path.join(dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# To save a bit of memory we can set text_X, reviewTraining, val_X, reviewTest and val_y to None.\n",
    "xTest = xTrain = xV = yTrain = yV = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#upload  to S3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Sagemaker can be accessed on 2 levels (High Level and Low level):\n",
    "- High Level: Less Flexibility to adapt model to specifics of the data given and the analysis one wants to perform, but more user comfort as the details of the implementation are run done in the background.\n",
    "- Low Level: For higher flexibility and adaptability it is required to know all objects Sagemaker has to instantiate to create, validate, test, use and store the model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import sagemaker\n",
    "\n",
    "# the session object allows usage of sagemakers methods\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# folder where model and data are stored\n",
    "prefix = 'sentiment-web-app'\n",
    "\n",
    "locTest = session.upload_data(os.path.join(dir, 'test.csv'), key_prefix=prefix)\n",
    "locV = session.upload_data(os.path.join(dir, 'validation.csv'), key_prefix=prefix)\n",
    "locTrain = session.upload_data(os.path.join(dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "prefix = 'sentiment-prediction-app'\n",
    "\n",
    "locTest = session.upload_data(os.path.join(dir, 'test.csv'), key_prefix=prefix)\n",
    "locV = session.upload_data(os.path.join(dir, 'validation.csv'), key_prefix=prefix)\n",
    "locTrain = session.upload_data(os.path.join(dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "In Sagemaker a so called model (inference rule) consists of 3 objects:\n",
    "- artefacts (parameters, i.e.values that change how a program/algorithm operates)\n",
    "- training container\n",
    "- prediction container\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#retrieve the location of the container (the training and inference code both use the same container)\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# get estimator object:\n",
    "#rt := random tree\n",
    "\n",
    "rtModel = sagemaker.estimator.Estimator(container, role, train_instance_count=1, train_instance_type='ml.m4.xlarge',      \n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# initialize with parameters:\n",
    "rtModel.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# get data and train the random tree model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=locTrain, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=locV, content_type='csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtModel.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Testing the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Batch Transform is a convenient way to perform inference on a large dataset in a way that is not realtime. That is, we don't necessarily need to use our model's results immediately and instead we can perform inference on a large number of samples.  it means that we can perform inference on our entire test set.\n",
    "To perform a Batch Transformation we need to first create a transformer objects from our trained estimator object.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtTransformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge') #rt := random tree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Doing the Transform job\n",
    "\n",
    "Because the test data is too large the data file has to be split up. \n",
    "Each line corresponds to a single sample, so it is possible to split the input on each line.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtTransformer.transform(locTest, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtTransformer.wait() # the wait method is used to get information on progress of the background processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#saved on S3\n",
    "#copy the file to the directory instead of working locally"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Definitions for output/prediction:\n",
    "0 := negative\n",
    "1 := positive) \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pred = pd.read_csv(os.path.join(dir, 'test.csv.out'), header=None)\n",
    "pred = [round(p) for p in pred.squeeze().values]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ratingTest, pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Deploying the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "The Sagemaker functionalities to store the parameters and deploy the inference code('endpoint') are used now; a link to the storage location on S3 is automatically provided afterwards. There are 2 important things to note:\n",
    "- A deployed model doesn't change the model parameters when test data is processed\n",
    "- The computation has to be stopped manually to avoid excessive costs for computing capacity.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtPredictor = rtModel.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge') #rt := random tree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Testing the model "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "After using the batch transform functionality of SageMaker to do the first test there will be another testrun using the newly created endpoint. As the amount of information that can be sent in each call is limited it is necessary to break the testing data up into chunks and then send each chunk. The serialization of the input data is done by Sagemaker.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# We need to tell the endpoint what format the data we are sending is in so that SageMaker can perform the serialization.\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "\n",
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    pred = ''\n",
    "    for array in split_array:\n",
    "        pred = ','.join([pred, xgb_predictor.predict(array).decode('utf-8')])\n",
    "    \n",
    "    return np.fromstring(pred[1:], sep=',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviewTest = pd.read_csv(os.path.join(dir, 'test.csv'), header=None).values\n",
    "pred = predict(reviewTest)\n",
    "pred = [round(p) for p in pred]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ratingTest, pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtPredictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Steps to  finally create the App:\n",
    "- create a new endpoint which does not require authentication and which acts as a proxy for the SageMaker endpoint.\n",
    "- the lambda functionality (function that can be executed whenever a specified event occurs) is used to do the preprocessing of the input data from the user in the backend after receiving it from a Sagemaker endpoint (a web adress), which is created by the API Gateway/Interface.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#for testing the model a random review for which a prediction will be made is used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "userInput = \"As I write this, I want to describe my raw initial state after I finished the film, I'm in a state of complete awe, staring into the wall kind of awe, Parasite is truly a work of art, a sheer masterpiece. This film oozes with mastery, every little detail tells a story of its own, I was drawn to it like a moth to a flame, it grips hard and it never lets go, it sways between genres gracefully, it offers comedy both dark and light, drama, horror, thrill, and it's all packaged so seamlessly, conveyed to us throughout breathtaking performances across the board, I've watched my share of Korean Cinema to know that's a common thing but Parasite takes it to a whole other level, it materializes thoughts and ideas, things words can not communicate, it is extremely rare to experience such a thing in film, I can only name a small number of movies that actually made me feel this way, just incredible. Director Bong Joon Ho proves that he is a master within his own cross-genre domain, he takes charge and you actually feel like you're in safe hands watching this, he shapes the scenes perfectly, A true master of his trade, Thank You. In all honesty, I feel like I went through an experiment where the time went still and I experienced a piece of art such as this, I absloutely, wholeheartedly loved every single second of this film, MINDBLOWING.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "wordvector = convert2words(userInput)\n",
    "print(userInput)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def getCountmatrix(words, vocab):\n",
    "    bow = [0] * len(vocab) # Start by setting the count for each word in the vocabulary to zero.\n",
    "    for word in words.split():  # For each word in the string\n",
    "        if word in vocab:  # If the word is one that occurs in the vocabulary, increase its count.\n",
    "            matrix01[vocab[word]] += 1\n",
    "    return matrix\n",
    "\n",
    "countMatrix = getCountmatrix(test_words, vocabulary)\n",
    "print(countMatrix)\n",
    "\n",
    "len(countMatrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# To send the input to the endpoint it is necessary to start the endpoint back up.\n",
    "rtPredictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge') # rt := random tree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "As the xgb_predictor object cannot be accessed to send the preprocessed input data to an endpoint it is necessary to find a different way to do that; Amazon provides the boto3 module for this:\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import boto3\n",
    "\n",
    "runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''make use of (invoke) an endpoint that has already been created. However, we need to provide SageMaker with the name of the deployed endpoint. To find this out we can print it out using the xgb_predictor object.\n",
    "'''\n",
    "\n",
    "rtPredictor.endpoint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#invoke the endpoint and send it the test_bow data\n",
    "\n",
    "responseDict = runtime.invoke_endpoint(EndpointName = rtPredictor.endpoint, ContentType = 'text/csv', Body = test_bow)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "responseDict = runtime.invoke_endpoint(EndpointName = rtPredictor.endpoint, ContentType = 'text/csv', Body = ','.join([str(d) for d in test_bow]).encode('utf-8'))\n",
    "print(responseDict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#the response responseDict is a dictionary; for further steps it has to be brought to a utf-8 format\n",
    "\n",
    "response = responseDict['Body'].read().decode('utf-8')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "For the production of an App there Lambda and API Gateway services can be used.\n",
    "\n",
    "1. Lambda is a functionality for reusable code.\n",
    "2. API Gateway is a service that allows you to create Web-URLs which are connected to other Amazon services. \n",
    "\n",
    "Overview: \n",
    "Data is sent from user - lambda function is triggered by public endpoint - data is given to the trained model's endpoint - prediction is given to the user.\n",
    "\n",
    "Setting up a Lambda function\n",
    "The  Lambda function will be executed when the public API gets input from a user; then it will receive the data, perform the preprocessing, send the input to the SageMaker endpoint and return the result.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import boto3\n",
    "import re\n",
    "\n",
    "replace = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "replaceWithSpacing = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def convert2words(review):\n",
    "    words = replace.sub(\"\", review.lower())\n",
    "    words = replaceWithSpacing.sub(\" \", words)\n",
    "    return words\n",
    "\n",
    "def getCountmatrix(words, vocab:\n",
    "    countmatrix = [0] * len(vocab) \n",
    "    for word in words.split():  \n",
    "        if word in vocab:  \n",
    "            countmatrix[vocab[word]] += 1\n",
    "    return countmatrix\n",
    "\n",
    "def ldfunction(event, context):\n",
    "\n",
    "    vocab = \"Vocabulary:\"\n",
    "\n",
    "    words = convert2words(event['body'])\n",
    "    countmatrix = getCountmatrix(words, vocab)\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    # use sm-runtime to invoke the endpoint / send the user input = review as .txt :\n",
    "    response = runtime.invoke_endpoint(EndpointName = rtPredictor.endpoint,\n",
    "                                       ContentType = 'text/csv',                 \n",
    "                                       Body = ','.join([str(val) for val in bow]).encode('utf-8')) \n",
    "\n",
    "    # HTTP response whose body contains the result \n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Round the result so that the app gets just '1' or '0'\n",
    "    result = round(float(result))\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : str(result)\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtPredictor.endpoint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "copy the vocabulary dict to the appropriate place in the code at the beginning of the ldfunction method. The cell below prints out the vocabulary dict in a way that is easy to copy and paste."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(str(vocab))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Setting up an API Gateway (creating an app to execute the lambda function) after having deployed the endpoint which communicates with the App\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Delete the endpoint to reduce costs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rtPredictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 2019)",
   "env": {
    "AR": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-ar",
    "AS": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-as",
    "CC": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-cc",
    "CONDA_EXE": "/ext/anaconda-2019.03/bin/conda",
    "CONDA_PREFIX": "/ext/anaconda-2019.03",
    "CONDA_PYTHON_EXE": "/ext/anaconda-2019.03/bin/python",
    "CPP": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-cpp",
    "CXX": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-c++",
    "CXXFILT": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-c++filt",
    "ELFEDIT": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-elfedit",
    "F77": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gfortran",
    "F90": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gfortran",
    "F95": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-f95",
    "FC": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gfortran",
    "GCC": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gcc",
    "GCC_AR": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gcc-ar",
    "GCC_NM": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gcc-nm",
    "GCC_RANLIB": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib",
    "GDAL_DATA": "/ext/anaconda-2019.03/share/gdal",
    "GFORTRAN": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gfortran",
    "GPROF": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-gprof",
    "GXX": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-g++",
    "JAVA_HOME": "/ext/anaconda-2019.03",
    "JAVA_LD_LIBRARY_PATH": "/ext/anaconda-2019.03/lib/server",
    "LD": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-ld",
    "LD_GOLD": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-ld.gold",
    "LD_LIBRARY_PATH": "/ext/anaconda-2019.03/lib",
    "NM": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-nm",
    "OBJCOPY": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-objcopy",
    "OBJDUMP": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-objdump",
    "OCAMLFIND_CONF": "/ext/anaconda-2019.03/etc/findlib.conf",
    "OCAMLLIB": "/ext/anaconda-2019.03/lib/ocaml",
    "OCAML_PREFIX": "/ext/anaconda-2019.03",
    "PATH": "/ext/anaconda-2019.03/bin:/ext/anaconda-2019.03/condabin:/cocalc/bin:/cocalc/src/smc-project/bin:/home/user/bin:/home/user/.local/bin:/ext/bin:/usr/lib/xpra:/opt/ghc/bin:/usr/local/bin:/usr/bin:/bin:/ext/data/homer/bin:/ext/data/weblogo:/ext/intellij/idea/bin:/ext/pycharm/pycharm/bin:/usr/lib/postgresql/10/bin",
    "PROJ_LIB": "/ext/anaconda-2019.03/share/proj",
    "PYTHONHOME": "/ext/anaconda-2019.03/lib/python3.7",
    "PYTHONPATH": "/ext/anaconda-2019.03/lib/python3.7:/ext/anaconda-2019.03/lib/python3.7/site-packages",
    "RANLIB": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-ranlib",
    "READELF": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-readelf",
    "RSTUDIO_WHICH_R": "/ext/anaconda-2019.03/bin/R",
    "SIZE": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-size",
    "STRINGS": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-strings",
    "STRIP": "/ext/anaconda-2019.03/bin/x86_64-conda_cos6-linux-gnu-strip"
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python/R distribution for data science",
     "priority": 5,
     "url": "https://www.anaconda.com/distribution/"
    }
   },
   "name": "anaconda2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}